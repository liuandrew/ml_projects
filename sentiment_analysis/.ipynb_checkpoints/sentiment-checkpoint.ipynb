{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torchtext.datasets import YelpReviewFull\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import dataset\n",
    "import math\n",
    "from typing import Tuple\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerSentiment(nn.Module):\n",
    "    def __init__(self, ntoken, d_model, nhead, d_hid, nlayers, num_classes=5, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoder(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.sentiment_ff = nn.Linear(d_model, num_classes)\n",
    "        \n",
    "        # self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def forward(self, src, src_mask=None):\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src.view([-1, 1, self.d_model]), src_mask)\n",
    "        output = self.sentiment_ff(output)\n",
    "        output = output.sum(dim=0).squeeze() / math.sqrt(src.size(0))\n",
    "        output = F.softmax(output, dim=0) #not sure whether to include softmax here\n",
    "        return output\n",
    "        \n",
    "\n",
    "device = torch.device('cpu')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x) -> Tensor:\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 650000/650000 [02:16<00:00, 4776.08it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 50000/50000 [00:09<00:00, 5149.85it/s]\n"
     ]
    }
   ],
   "source": [
    "train = YelpReviewFull(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, map(lambda x: x[1], train)))\n",
    "\n",
    "vocab.set_default_index(0)\n",
    "\n",
    "def data_process(text_iter):\n",
    "    '''\n",
    "    Process data from YelpReviewFull\n",
    "    Return scores and tokenized text split into two tensors\n",
    "    '''\n",
    "    text_data = []\n",
    "    score_data = []\n",
    "    for item in tqdm(text_iter):\n",
    "        text_data.append(torch.tensor(vocab(tokenizer(item[1])), dtype=torch.long))\n",
    "        score_data.append(torch.tensor([[item[0]]], dtype=torch.long))\n",
    "        \n",
    "    score_data = torch.cat(tuple(filter(lambda t: t.numel() > 0, score_data)))\n",
    "    score_data = F.one_hot(score_data - 1).squeeze() #convert to one hot for comparison\n",
    "    \n",
    "    return score_data, text_data\n",
    "\n",
    "train_iter, test_iter = YelpReviewFull()\n",
    "train_data = data_process(train_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "valid_data = (train_data[0][0:20000], train_data[1][0:20000])\n",
    "train_data = (train_data[0][20000:], train_data[1][20000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab)\n",
    "# ntokens = 5000\n",
    "emsize = 200 #same as d_model\n",
    "d_hid = 200 #FF network width\n",
    "nlayers = 2 #number of attention layers\n",
    "nhead = 2 #number of heads per attention layer\n",
    "dropout = 0.2\n",
    "model = TransformerSentiment(ntokens, emsize, nhead, d_hid, nlayers, dropout=dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20621405"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = 32\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "def get_batch(data, i=0):\n",
    "    return data[0][i:i+bsz], data[1][i:i+bsz]\n",
    "\n",
    "\n",
    "\n",
    "def train(model):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(train_data[1]) // bsz\n",
    "\n",
    "    for batch, i in enumerate(tqdm(range(0, len(train_data[1]) - 1, bsz))):\n",
    "        targets, inputs = get_batch(train_data, i)\n",
    "\n",
    "        outputs = []\n",
    "        for i in range(len(inputs)):\n",
    "            output = model(inputs[i])\n",
    "            outputs.append(output)\n",
    "        outputs = torch.stack(outputs)\n",
    "        loss = criterion(outputs, targets.type(torch.FloatTensor))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f' epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches'\n",
    "                        f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                        f'loss {cur_loss:5.2f} | ppl{ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "            \n",
    "            \n",
    "def evaluate(model, eval_data):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(eval_data[1]) - 1, bsz):\n",
    "            targets, inputs = get_batch(eval_data, i)\n",
    "            outputs = []\n",
    "            for i in range(len(inputs)):\n",
    "                output = model(inputs[i])\n",
    "                outputs.append(output)\n",
    "            outputs = torch.stack(outputs)\n",
    "            \n",
    "            total_loss += criterion(outputs, targets)\n",
    "    return total_loss / len(eval_data[1])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs = 3\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model)\n",
    "    val_loss = evaluate(model, valid_data)\n",
    "    val_ppl = math.exp(val_loss)\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print('-'*89)\n",
    "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s |'\n",
    "        f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "    print('-'*89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        torch.save(model, 'best_model.pt')\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = IMDB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data[0]:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('neg',\n",
       " \"Whoever wrote the screenplay for this movie obviously never consulted any books about Lucille Ball, especially her autobiography. I've never seen so many mistakes in a biopic, ranging from her early years in Celoron and Jamestown to her later years with Desi. I could write a whole list of factual errors, but it would go on for pages. In all, I believe that Lucille Ball is one of those inimitable people who simply cannot be portrayed by anyone other than themselves. If I were Lucie Arnaz and Desi, Jr., I would be irate at how many mistakes were made in this film. The filmmakers tried hard, but the movie seems awfully sloppy to me.\")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = IMDB(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, map(lambda x: x[1], train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100682\n"
     ]
    }
   ],
   "source": [
    "vocab.set_default_index(0)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_to_idx = {\n",
    "    'neg': 0,\n",
    "    'pos': 1\n",
    "}\n",
    "\n",
    "def data_process(text_iter):\n",
    "    text_data = []\n",
    "    score_data = []\n",
    "    for item in tqdm(text_iter):\n",
    "        text_data.append(torch.tensor(vocab(tokenizer(item[1])), dtype=torch.long))\n",
    "        score_data.append(torch.tensor([[score_to_idx[item[0]]]], dtype=torch.long))\n",
    "    score_data = torch.cat(tuple(filter(lambda t: t.numel() > 0, score_data)))\n",
    "    score_data = F.one_hot(score_data - 1).squeeze() #convert to one hot for comparison\n",
    "    \n",
    "    return score_data, text_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 650000/650000 [02:16<00:00, 4776.08it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 50000/50000 [00:09<00:00, 5149.85it/s]\n"
     ]
    }
   ],
   "source": [
    "train = YelpReviewFull(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, map(lambda x: x[1], train)))\n",
    "\n",
    "vocab.set_default_index(0)\n",
    "\n",
    "def data_process(text_iter):\n",
    "    '''\n",
    "    Process data from YelpReviewFull\n",
    "    Return scores and tokenized text split into two tensors\n",
    "    '''\n",
    "    text_data = []\n",
    "    score_data = []\n",
    "    for item in tqdm(text_iter):\n",
    "        text_data.append(torch.tensor(vocab(tokenizer(item[1])), dtype=torch.long))\n",
    "        score_data.append(torch.tensor([[item[0]]], dtype=torch.long))\n",
    "        \n",
    "    score_data = torch.cat(tuple(filter(lambda t: t.numel() > 0, score_data)))\n",
    "    score_data = F.one_hot(score_data - 1).squeeze() #convert to one hot for comparison\n",
    "    \n",
    "    return score_data, text_data\n",
    "\n",
    "train_iter, test_iter = YelpReviewFull()\n",
    "train_data = data_process(train_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "valid_data = (train_data[0][0:20000], train_data[1][0:20000])\n",
    "train_data = (train_data[0][20000:], train_data[1][20000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = nn.Embedding(1000, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5431e+00, -1.0361e+00,  1.7384e+00,  5.7873e-01, -5.0230e-01,\n",
       "          1.5696e-01,  1.0780e+00,  1.9812e+00, -8.7044e-01, -3.3446e-01,\n",
       "          1.5135e+00, -9.5995e-01, -4.2504e-01, -2.5587e-02, -1.4463e+00,\n",
       "          2.3722e-01,  1.7607e-01, -1.0944e+00, -4.5529e-01,  8.5847e-01,\n",
       "         -5.2575e-01,  1.5628e+00, -7.7671e-01,  6.8639e-01, -2.3997e-02,\n",
       "          1.1447e+00, -3.3775e-01,  5.4042e-02,  8.5677e-01,  6.3472e-02,\n",
       "         -1.7705e-02, -9.7236e-01,  3.4769e-01, -1.1070e+00,  8.1678e-01,\n",
       "          4.2904e-02,  1.9631e+00,  1.7294e+00, -1.1673e-01, -1.5088e+00,\n",
       "         -1.5890e-01, -3.2197e-01,  4.5689e-01, -4.8750e-01,  4.2802e-02,\n",
       "         -2.8633e-01, -1.5443e+00, -4.8136e-01,  2.8803e+00, -7.3198e-02,\n",
       "         -3.8879e-01, -6.7012e-01,  1.8961e-01, -2.7022e-01, -4.5269e-01,\n",
       "         -1.6747e+00, -3.4815e-01,  1.1421e+00, -5.5445e-02, -2.8550e-01,\n",
       "         -8.3498e-01,  1.0001e+00,  4.8800e-01,  3.4807e-01,  9.7923e-01,\n",
       "          1.1082e-01, -1.1748e+00, -4.2528e-01,  1.7352e-01,  6.8216e-01,\n",
       "          2.2085e+00, -2.1551e+00,  2.2021e+00, -6.7971e-01, -2.2801e-02,\n",
       "         -4.8559e-01,  4.8840e-01,  1.0343e+00, -2.7705e-01, -2.8258e-01,\n",
       "         -1.4910e-01,  6.8978e-01,  1.7204e+00,  1.2053e+00, -1.0866e+00,\n",
       "         -2.5952e+00,  7.4701e-01, -4.5337e-01, -1.4238e+00,  7.3725e-01,\n",
       "          7.1998e-01, -6.0391e-01,  2.3625e-01,  1.4262e+00, -1.8186e+00,\n",
       "         -4.6450e-01,  1.1485e+00,  7.2831e-01,  4.6840e-01, -6.9577e-01,\n",
       "          2.0425e-01, -1.2045e+00, -5.6348e-02,  1.9160e+00, -8.8255e-01,\n",
       "          2.6805e+00,  4.1180e-01, -3.0044e-01,  2.6532e-01, -2.0334e-01,\n",
       "          1.5134e-02, -9.4495e-02,  6.7416e-01, -5.9721e-01, -1.1712e+00,\n",
       "          1.4971e+00,  5.5239e-01, -3.2293e-01, -3.6599e-01, -2.4522e+00,\n",
       "          9.4717e-01, -4.5304e-01, -4.6791e-01,  5.2903e-01,  6.8935e-01,\n",
       "         -2.7996e-01,  1.8549e-01,  8.8722e-01,  1.4324e+00,  1.1375e+00,\n",
       "          8.9706e-01,  7.7231e-01, -6.1322e-01,  1.1208e-01, -2.3969e-01,\n",
       "          9.5879e-01, -6.5251e-01, -1.2576e+00,  5.5490e-01,  1.0800e+00,\n",
       "          3.1545e-01, -1.5433e+00, -2.0019e-01, -8.8659e-01, -1.0658e+00,\n",
       "         -1.8121e-01, -1.7234e+00,  1.7292e+00,  1.5950e+00, -8.1193e-01,\n",
       "          2.0132e+00,  9.4225e-01,  7.6335e-01,  4.5923e-01,  7.3232e-02,\n",
       "          1.0058e+00,  7.8645e-01,  9.5842e-02, -8.9542e-01,  1.0439e+00,\n",
       "          2.4543e-01, -7.0728e-01,  9.9041e-01, -3.7128e-01, -1.3192e+00,\n",
       "         -2.3921e-01, -2.0787e-01,  1.0571e+00, -1.2604e-01, -7.2240e-01,\n",
       "          1.9429e-03,  4.5528e-01,  4.9217e-01, -1.2816e-01, -1.4911e+00,\n",
       "          1.1933e+00, -1.5182e-01, -1.0113e-01,  2.7619e-02, -3.6960e-01,\n",
       "          6.8186e-01,  3.1491e-01, -1.4305e+00,  1.0682e+00,  8.0837e-01,\n",
       "         -2.1794e+00, -6.0231e-01,  1.9613e+00, -2.5857e-01, -2.5122e-01,\n",
       "          5.7944e-01,  9.6748e-01,  2.1499e-01,  2.7875e-01, -3.8724e-01,\n",
       "         -1.3592e+00,  1.2223e+00, -4.9533e-01, -3.3190e-01,  2.6806e-01],\n",
       "        [-9.7385e-01,  9.5628e-01,  1.6264e+00,  4.1861e-01, -3.0480e-01,\n",
       "         -1.6814e-01,  2.0779e-01,  7.1907e-01,  7.4814e-01, -1.7878e+00,\n",
       "         -9.9477e-01,  5.6894e-02, -6.3640e-01, -1.7327e+00,  9.4928e-01,\n",
       "          1.8638e+00, -5.3025e-01,  8.3809e-01,  2.0639e+00,  1.8054e+00,\n",
       "          2.5774e-01,  1.6671e-01,  8.2397e-01,  5.3618e-01,  1.0025e+00,\n",
       "         -2.2733e-01,  5.8304e-01,  8.0166e-01,  7.4571e-01,  2.9375e-01,\n",
       "         -9.6938e-01,  1.4424e+00,  1.2981e+00,  7.7757e-01,  1.3062e+00,\n",
       "         -8.5808e-01,  2.3760e+00,  2.7975e-01,  4.2849e-01,  6.5539e-01,\n",
       "          2.0582e+00, -1.4650e+00,  1.0180e+00,  4.9102e-01, -7.2258e-01,\n",
       "          6.6337e-01,  1.0523e-01,  1.5066e+00,  3.4895e-01,  7.1867e-01,\n",
       "         -8.5280e-01, -8.1143e-01, -1.3941e-01,  9.0177e-01,  1.5026e+00,\n",
       "         -9.1111e-01,  7.4764e-01,  2.3722e-01,  2.4064e+00,  1.3802e+00,\n",
       "          3.0153e-02,  1.0322e+00,  6.9650e-01,  5.0389e-01,  1.2070e+00,\n",
       "          1.3908e+00,  6.5650e-01, -1.1415e-01, -1.0642e+00, -8.9139e-02,\n",
       "         -2.3536e-02,  4.6250e-01,  4.0958e-01,  9.4842e-01, -9.7987e-02,\n",
       "          3.2444e-01, -3.1570e-01,  4.0353e-01, -5.3475e-01, -8.2723e-01,\n",
       "          1.6460e+00,  1.3772e+00,  4.7131e-01,  1.8755e-01,  1.6613e+00,\n",
       "         -5.6209e-01,  1.2882e+00, -2.7451e-02,  1.8308e+00, -6.6060e-01,\n",
       "          1.9179e+00,  5.4314e-01, -7.5693e-01,  2.2007e-01, -4.4270e-01,\n",
       "         -9.1046e-01,  2.2244e-01,  1.0145e-01,  1.3524e+00, -1.4763e-01,\n",
       "          4.5112e-01, -5.6325e-01, -1.2485e+00, -1.3551e-01, -6.4458e-01,\n",
       "          6.3401e-01,  9.6693e-01,  1.5973e+00,  1.4831e+00, -2.1249e+00,\n",
       "         -7.8754e-01, -4.9764e-01, -1.2569e+00,  8.0738e-01, -5.2711e-01,\n",
       "          7.4101e-01, -4.7131e-01, -2.5697e-01,  7.2627e-01, -2.0687e-01,\n",
       "         -3.7068e-01, -4.7119e-01,  8.2488e-02,  1.1423e+00,  8.8210e-01,\n",
       "         -1.7216e+00,  6.4431e-01, -2.1364e-02, -3.2135e-01,  9.0339e-01,\n",
       "          5.9348e-01, -1.2730e+00, -4.9975e-01, -6.1709e-01, -9.1619e-01,\n",
       "         -7.7936e-01,  9.8364e-01,  4.2526e-01, -2.9909e-01, -1.4695e-01,\n",
       "         -7.4925e-01, -1.0977e-01,  4.7224e-01, -6.1576e-01, -2.0553e+00,\n",
       "         -4.2760e-01, -4.6035e-02, -9.6382e-01,  1.2857e+00,  7.4557e-01,\n",
       "          4.2754e-02, -8.5139e-01,  2.2930e-02,  3.1682e-01, -2.1046e+00,\n",
       "         -1.2436e+00, -1.5080e+00,  1.8393e+00, -4.7177e-01,  2.4883e-01,\n",
       "         -2.3467e+00, -7.3597e-01, -3.7753e-02, -6.3904e-01,  1.0594e-01,\n",
       "         -1.0300e+00, -1.1190e+00,  3.9617e-01, -3.1211e-01, -1.8666e-01,\n",
       "          1.4368e+00, -2.0919e+00, -1.6643e+00,  1.1080e+00, -2.3994e+00,\n",
       "         -6.8532e-01, -1.1270e+00, -5.1677e-02,  1.7559e+00,  1.8164e-01,\n",
       "          1.7314e+00,  9.8225e-02,  1.6474e+00,  4.5774e-01, -3.6079e-01,\n",
       "         -4.2731e-01,  1.0186e+00,  3.4185e-01, -1.0084e+00, -1.3055e-01,\n",
       "         -9.4946e-01,  1.7245e+00,  6.0524e-03,  1.6923e+00, -1.5480e+00,\n",
       "          1.0845e-01, -7.2261e-01, -6.6787e-01,  4.3108e-01, -1.1856e+00],\n",
       "        [-1.4585e+00, -6.1761e-01,  5.8661e-01, -8.3189e-01,  3.1207e-01,\n",
       "         -3.9081e-01, -2.0173e-01, -3.1692e-01, -5.6327e-01, -1.1472e-01,\n",
       "         -1.1707e+00, -3.7151e-02, -1.9069e+00, -1.0630e+00,  1.4320e+00,\n",
       "         -4.7122e-01,  1.4818e-01, -4.1292e-02, -5.2764e-01, -6.9420e-02,\n",
       "          1.2654e-01, -5.4021e-01,  1.8340e+00, -1.1337e+00, -5.7281e-01,\n",
       "         -1.7831e-01,  4.6923e-01, -1.9675e-01,  1.4554e+00,  2.0626e-01,\n",
       "          5.3657e-01,  8.5540e-01,  7.2878e-01, -1.0859e+00,  6.1540e-01,\n",
       "         -1.1458e+00, -1.0778e+00,  5.7816e-02, -8.5359e-01,  2.9825e-02,\n",
       "          1.4355e+00, -2.7606e+00,  2.2372e-01, -8.4984e-02, -6.4198e-01,\n",
       "         -4.5299e-01, -1.6881e+00, -1.9389e+00, -3.5747e-01, -1.4889e-01,\n",
       "         -5.5861e-01,  1.9058e-01, -2.5921e-02,  4.3339e-01,  1.4045e+00,\n",
       "         -2.3578e-01, -4.9345e-01, -3.0245e-01, -4.0941e-02,  1.1727e+00,\n",
       "         -1.1114e+00,  1.0308e-01,  1.6165e+00,  1.4447e-01,  1.6618e-01,\n",
       "          5.4451e-01, -9.0493e-01,  4.0369e-01, -1.7201e-02, -4.8927e-01,\n",
       "          7.5134e-01,  3.2610e-01,  7.7311e-01,  1.1687e+00, -1.2550e+00,\n",
       "         -5.2640e-01,  4.8727e-03,  9.2485e-02, -2.5969e+00, -3.8910e-01,\n",
       "         -1.8084e-01, -1.4742e+00, -7.3684e-01, -5.6050e-01, -2.3767e+00,\n",
       "         -1.6568e-01, -2.9848e-01,  1.7304e+00,  1.0072e+00, -1.4474e+00,\n",
       "          5.5021e-02, -1.9146e+00,  1.2318e+00,  5.9707e-01, -1.2621e+00,\n",
       "         -1.6678e+00, -6.8276e-02,  7.1766e-01, -1.8255e+00, -1.4517e-01,\n",
       "         -2.1963e-01,  2.1783e-01, -1.3127e+00,  2.8178e+00, -1.1663e+00,\n",
       "         -1.2888e-01,  4.4775e-01, -1.3751e+00,  5.1799e-01, -1.2879e-01,\n",
       "          4.0185e-01, -1.6510e+00, -2.9355e-01, -1.2809e+00,  2.5672e+00,\n",
       "          1.3286e+00,  1.3282e+00, -6.8380e-01, -2.1751e+00, -6.1469e-01,\n",
       "         -8.4574e-01, -1.0655e+00,  1.9709e-01, -1.4053e+00,  1.6031e+00,\n",
       "          1.6905e-01, -2.1691e-01,  3.1998e-01,  6.1755e-01,  1.1290e+00,\n",
       "         -1.5107e+00,  1.2546e+00, -1.4935e+00,  4.2818e-01, -5.3937e-02,\n",
       "         -5.6361e-01, -2.0189e-01, -5.2322e-01, -5.0804e-01, -1.2446e+00,\n",
       "         -2.5124e+00,  1.7961e+00,  9.0448e-01, -2.1479e+00,  4.3490e-01,\n",
       "         -9.1805e-01,  1.4272e+00,  7.3681e-01, -3.0328e-01, -3.7299e-01,\n",
       "          2.4960e-01,  4.8473e-01, -1.0212e+00, -1.1452e+00, -3.1645e-02,\n",
       "         -2.1640e-01, -2.3974e-01,  5.7149e-01, -6.5446e-01, -7.2557e-01,\n",
       "          9.9454e-02, -4.5704e-01, -8.9364e-01, -5.7216e-01, -1.4806e+00,\n",
       "          1.0724e+00,  9.6961e-01,  1.8949e+00,  1.3751e+00, -3.2304e-01,\n",
       "          3.6249e-01, -8.1448e-02,  1.5624e+00, -5.0810e-01, -8.0210e-01,\n",
       "         -4.9871e-01, -2.7660e-01, -2.4141e-01,  5.0954e-01,  5.9095e-01,\n",
       "          6.3879e-01, -1.3130e+00, -2.8617e-01,  7.8120e-01,  2.2257e-01,\n",
       "          2.2071e-01, -1.3793e+00, -5.5315e-01,  1.3189e+00,  2.2610e-02,\n",
       "          7.3799e-01,  3.3561e-01,  1.5605e-01, -3.0574e+00,  8.9629e-02,\n",
       "         -2.4698e-02, -2.5780e-01, -1.4401e+00,  2.1140e-01, -8.1839e-01],\n",
       "        [-1.3321e+00, -6.5290e-01,  2.2423e+00,  2.0275e-01, -1.1338e+00,\n",
       "         -2.2442e-01, -2.0649e+00,  1.8048e-01,  7.9300e-01,  5.3091e-01,\n",
       "         -2.3151e-01,  4.0159e-01,  1.0269e+00, -1.9668e+00,  1.1664e-01,\n",
       "         -3.3854e-01, -2.0474e-01, -5.6770e-01,  6.3604e-01,  3.0796e-01,\n",
       "          4.8721e-02, -8.0228e-01,  6.4836e-01, -3.9244e-01, -1.6045e+00,\n",
       "          3.8093e-02,  1.2224e-01, -1.2654e+00, -1.7695e+00, -7.0784e-01,\n",
       "          1.7552e+00, -3.1183e-02, -3.6050e-01,  1.1415e+00, -1.5371e+00,\n",
       "         -4.2273e-01,  1.9509e-01,  1.7413e-01, -7.2618e-01,  1.3289e+00,\n",
       "         -5.4266e-01,  6.9561e-01,  1.2654e+00, -4.4679e-01,  8.9411e-01,\n",
       "         -5.5650e-01,  2.6944e+00,  9.4101e-01, -1.5143e-02,  8.1051e-01,\n",
       "          1.0679e+00,  1.9186e+00,  2.2544e+00,  3.1802e-01,  2.4407e+00,\n",
       "          1.8505e+00, -3.7848e-02, -1.2400e-01,  5.8693e-01,  7.0527e-01,\n",
       "         -2.6559e+00, -4.4139e-01,  1.2514e+00,  3.6276e-01,  1.1989e+00,\n",
       "         -9.8612e-01,  1.3111e+00, -5.0907e-01, -5.9818e-01,  7.2141e-01,\n",
       "         -1.5235e-01, -6.7442e-01, -5.1067e-01, -8.4544e-01, -1.8702e+00,\n",
       "         -5.3270e-01,  7.7027e-01, -4.4062e-01,  1.4810e+00, -3.9603e-01,\n",
       "         -1.5504e+00, -1.9751e+00,  1.4947e-01, -5.0200e-01, -1.0658e-01,\n",
       "          1.8128e+00, -1.1743e+00,  1.1589e+00,  2.6816e+00, -1.4627e+00,\n",
       "         -8.4647e-01, -5.8147e-01, -2.2930e+00,  2.5021e-01, -1.7148e-01,\n",
       "          1.0624e-01, -9.6684e-01, -8.2096e-01, -1.2744e+00,  7.4805e-01,\n",
       "          5.6727e-01, -1.0201e-01, -5.2215e-01,  9.3085e-01,  3.7368e-01,\n",
       "          1.2042e+00,  3.2163e-01,  2.6880e-02,  1.2623e+00,  5.7473e-01,\n",
       "          6.0519e-01,  1.1045e+00,  7.8085e-02,  1.9993e-01, -9.6815e-01,\n",
       "          2.3853e-01,  2.0036e+00,  2.5903e+00,  1.9209e-01, -5.0151e-01,\n",
       "          8.2512e-01, -1.1703e+00,  1.9348e-02, -1.8181e+00, -2.3943e-01,\n",
       "         -3.1837e-01,  1.7462e+00,  1.1622e+00, -1.2377e+00,  1.6777e+00,\n",
       "         -3.6240e-01,  9.0427e-01,  4.9404e-01,  6.9653e-01,  3.6154e-01,\n",
       "          1.6294e+00, -6.2838e-01, -2.0008e+00, -2.5832e-01, -1.4615e+00,\n",
       "         -1.2918e-01,  7.5967e-01,  2.0840e-01, -9.1141e-01, -9.9545e-01,\n",
       "          1.4321e+00,  1.9041e+00,  1.7495e+00, -1.3520e+00,  1.0750e-01,\n",
       "         -1.8223e+00, -1.6967e+00,  4.9972e-02, -7.6570e-01, -9.8230e-01,\n",
       "          9.6894e-01, -2.2195e-01,  6.5219e-01, -4.6330e-01, -1.8023e-01,\n",
       "          1.6296e-01,  1.1651e+00, -3.7897e-01, -1.9469e-01, -1.1524e+00,\n",
       "          4.5130e-01,  4.1668e-01, -1.2700e+00,  1.1734e+00, -1.8278e+00,\n",
       "          1.2507e+00, -2.6175e-01,  1.6321e+00, -1.1131e+00,  5.5175e-01,\n",
       "          1.1495e+00, -2.6393e+00, -1.4882e-01,  4.2219e-01,  1.4656e+00,\n",
       "         -4.4129e-01,  4.5320e-02,  1.0037e+00, -6.3317e-01,  5.1912e-01,\n",
       "         -5.1888e-01,  6.3645e-01, -6.9404e-01,  5.9708e-01, -2.1102e+00,\n",
       "         -2.0163e+00,  1.0682e+00,  1.0977e-01,  1.6923e-01, -2.8194e-01,\n",
       "          3.2202e-01,  2.0966e+00, -4.7253e-01, -3.5167e-01,  2.2106e-01]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed(torch.tensor([0, 1, 2, 3], dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache\\glove.42B.300d.zip: 1.88GB [05:54, 5.30MB/s]                                                             \n",
      "100%|█████████████████████████████████████████████████████████████████████▉| 1917493/1917494 [10:32<00:00, 3033.72it/s]\n"
     ]
    }
   ],
   "source": [
    "fasttext_vectors = GloVe('42B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1917494"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fasttext_vectors.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(1917494, 300)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings = torch.nn.Embedding.from_pretrained(fasttext_vectors.vectors, freeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerSentiment(nn.Module):\n",
    "    def __init__(self, ntoken, d_model, nhead, d_hid, nlayers, num_classes=5, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoder(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = torch.nn.Embedding.from_pretrained(fasttext_vectors.vectors, freeze=True)\n",
    "        self.d_model = d_model\n",
    "        self.sentiment_ff = nn.Linear(d_model, num_classes)\n",
    "        \n",
    "        # self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def forward(self, src, src_mask=None):\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src.view([-1, 1, self.d_model]), src_mask)\n",
    "        output = self.sentiment_ff(output)\n",
    "        output = output.sum(dim=0).squeeze() / math.sqrt(src.size(0))\n",
    "        output = F.softmax(output, dim=0) #not sure whether to include softmax here\n",
    "        return output\n",
    "        \n",
    "\n",
    "device = torch.device('cpu')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fasttext_vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-5269513dc560>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtransformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTransformerSentiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfasttext_vocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'fasttext_vocab' is not defined"
     ]
    }
   ],
   "source": [
    "transformer = TransformerSentiment(len(fasttext_vocab), 300, 2, 300, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
